{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://dashscope.aliyuncs.com/compatible-mode/v1 qwen-plus\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "try:\n",
    "    # load environment variables from .env file (requires `python-dotenv`)\n",
    "    from dotenv import load_dotenv\n",
    "\n",
    "    load_dotenv()\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "if \"LANGSMITH_API_KEY\" not in os.environ:\n",
    "    os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass(\n",
    "        prompt=\"Enter your LangSmith API key (optional): \"\n",
    "    )\n",
    "if \"LANGSMITH_PROJECT\" not in os.environ:\n",
    "    os.environ[\"LANGSMITH_PROJECT\"] = getpass.getpass(\n",
    "        prompt='Enter your LangSmith Project Name (default = \"default\"): '\n",
    "    )\n",
    "    if not os.environ.get(\"LANGSMITH_PROJECT\"):\n",
    "        os.environ[\"LANGSMITH_PROJECT\"] = \"default\"\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\n",
    "        prompt=\"Enter your OpenAI API key (required if using OpenAI): \"\n",
    "    )\n",
    "\n",
    "base_url = os.environ.get(\"BASE_URL\")\n",
    "model_name = os.environ.get(\"MODEL_NAME\")\n",
    "deepseek_model_name = os.environ.get(\"DEEPSEEK_MODEL_NAME\")\n",
    "\n",
    "print(base_url, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_deepseek import ChatDeepSeek\n",
    "\n",
    "\n",
    "llm = ChatDeepSeek(\n",
    "    model=deepseek_model_name,\n",
    "    temperature=0,\n",
    "    max_retries=2,\n",
    "    base_url=base_url,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! 😊 How can I assist you today?'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm.invoke(\"Hello\")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"Get the weather from a location.\"\"\"\n",
    "\n",
    "    return \"Sunny.\"\n",
    "\n",
    "\n",
    "llm_with_tools = llm.bind_tools([get_weather])\n",
    "\n",
    "response = llm_with_tools.invoke(\"What's the weather in San Francisco, CA?\")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool result: Sunny.\n"
     ]
    }
   ],
   "source": [
    "# 检查是否有工具调用\n",
    "if response.tool_calls:\n",
    "    # 执行工具调用\n",
    "    tool_call = response.tool_calls[0]\n",
    "    result = get_weather.invoke(tool_call[\"args\"])\n",
    "    print(\"Tool result:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "chain = llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! 😊 How can I assist you today?'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke(\"Hello\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|I| can|'t| fetch| real|-time| weather| data|,| but| you| can| check| the| current| weather| in| **|San| Francisco|,| CA|**| using| services| like|:|  \n",
      "\n",
      "|-| [|Weather|.com|](|https|://|weather|.com|)|  \n",
      "|-| [|Acc|u|Weather|](|https|://|www|.|acc|uwe|ather|.com|)|  \n",
      "|-| [|National| Weather| Service|](|https|://|www|.|weather|.gov|)|  \n",
      "|-| A| quick| search| on| Google| (\"|weather| in| San| Francisco|\")|  \n",
      "\n",
      "|Would| you| like| general| climate| information| for| San| Francisco| instead|?| It| typically| has| mild| temperatures|,| frequent| fog| (|especially| in| summer|),| and| cool| bree|zes| from| the| Pacific|.| Let| me| know| how| I| can| help|!| 🌁|🌤|️||"
     ]
    }
   ],
   "source": [
    "for chunk in chain.stream(\"What's the weather in San Francisco, CA?\"):\n",
    "    print(chunk, end=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"anthropic:claude-3-5-sonnet-latest\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools = model.bind_tools([get_weather])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=[{'text': \"I'll check the weather in San Francisco for you.\", 'type': 'text'}, {'id': 'toolu_013Y8pTbRWpqrR4aWDmGQc8U', 'input': {'location': 'San Francisco, CA'}, 'name': 'get_weather', 'type': 'tool_use'}], additional_kwargs={}, response_metadata={'id': 'msg_01VCmBaAdmdbtJyfZCT4qST9', 'model': 'claude-3-5-sonnet-20241022', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 384, 'output_tokens': 68, 'server_tool_use': None, 'service_tier': 'standard'}, 'model_name': 'claude-3-5-sonnet-20241022'}, id='run--7493701a-f813-4a4f-8118-650cad05af0b-0', tool_calls=[{'name': 'get_weather', 'args': {'location': 'San Francisco, CA'}, 'id': 'toolu_013Y8pTbRWpqrR4aWDmGQc8U', 'type': 'tool_call'}], usage_metadata={'input_tokens': 384, 'output_tokens': 68, 'total_tokens': 452, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_with_tools.invoke(\"What's the weather in San Francisco, CA?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型差异对比\n",
    "DeepSeek 模型的行为：\n",
    "```python\n",
    "# DeepSeek 的响应\n",
    "response.content = \"\"  # 空字符串\n",
    "response.tool_calls = [{'name': 'get_weather', 'args': {...}}]\n",
    "```\n",
    "Claude 模型的行为：\n",
    "```python\n",
    "# Claude 的响应  \n",
    "response.content = [\n",
    "    {'text': \"I'll check the weather in San Francisco for you.\", 'type': 'text'},\n",
    "    {'id': '...', 'input': {...}, 'name': 'get_weather', 'type': 'tool_use'}\n",
    "]\n",
    "response.tool_calls = [{'name': 'get_weather', 'args': {...}}]\n",
    "```\n",
    "###关键区别\n",
    "- DeepSeek：当决定调用工具时，完全不返回文本内容，只生成工具调用\n",
    "- Claude：既提供解释性文本（\"I'll check the weather...\"），又生成工具调用"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
